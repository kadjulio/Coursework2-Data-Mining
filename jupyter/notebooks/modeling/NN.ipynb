{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import statements\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "        https://stackoverflow.com/a/50386871\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/processed/full_set.pkl', 'rb') as f:\n",
    "    full_set = pickle.load(f)\n",
    "with open('../../data/processed/train_set.pkl', 'rb') as f:\n",
    "    train_set = pickle.load(f)\n",
    "with open('../../data/processed/test_set.pkl', 'rb') as f:\n",
    "    test_set = pickle.load(f)\n",
    "with open('../../data/processed/train_set_30.pkl', 'rb') as f:\n",
    "    train_set_30 = pickle.load(f)\n",
    "with open('../../data/processed/test_set_30.pkl', 'rb') as f:\n",
    "    test_set_30 = pickle.load(f)\n",
    "with open('../../data/processed/train_set_70.pkl', 'rb') as f:\n",
    "    train_set_70 = pickle.load(f)\n",
    "with open('../../data/processed/test_set_70.pkl', 'rb') as f:\n",
    "    test_set_70 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_70.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X, y format \n",
    "X_train = train_set.T.iloc[:-1].T\n",
    "y_train = train_set.T.iloc[-1].T\n",
    "\n",
    "X_test = test_set.T.iloc[:-1].T\n",
    "y_test = test_set.T.iloc[-1].T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm a little suspicious about how complicated the linear kernel is - possibly more than we need.\n",
    "# A multilayer perceptrol with 0 layers is also a linear classifier if we need it.\n",
    "from sklearn.svm import SVC\n",
    "linclf = SVC(kernel=\"linear\")\n",
    "linclf.fit(X_train, y_train) \n",
    "linclf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "mlp = MLPClassifier(random_state=42\n",
    "                    # more non default parameters? E.g. only stochastic gradient descent has been covered in lectures \n",
    "                    # also doesn't reach convergence before timeout with current settings\n",
    "                   )\n",
    "# mlp.fit(X_train, y_train) \n",
    "# mlp.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo run this\n",
    "def cross_validate(classifier, X, y):\n",
    "    '''    \n",
    "    Given a classifier and training data:\n",
    "        * Do 10fold CV\n",
    "        * average the scores\n",
    "    What this means is for the caller to interpret.\n",
    "    Returns average result over CV runs \n",
    "    '''\n",
    "    standardising_classifier = make_pipeline(preprocessing.StandardScaler(), classifier)\n",
    "    cross_val_score(standardising_classifier, X, y, cv=10)\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running An Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "\n",
    "def run_experiment(classifier, X_train_data, y_train_data, X_test_data, y_test_data):\n",
    "    '''\n",
    "    Given a classifier, training data, and test data:\n",
    "    * Train on the training data\n",
    "    * Test on the test data\n",
    "    * display the confusion matrix and other metrics\n",
    "    '''\n",
    "    classifier_pipe = make_pipeline(preprocessing.StandardScaler(), classifier)\n",
    "\n",
    "    classifier_pipe.fit(X_train_data, y_train_data)\n",
    "    y_pred = classifier_pipe.predict(X_test_data)\n",
    "\n",
    "    conf_mat = confusion_matrix(y_test_data, y_pred)\n",
    "    print(classification_report(y_test_data, y_pred))\n",
    "    plot_confusion_matrix(conf_mat, target_names=y_test_data.unique().sort())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original split\n",
    "run_experiment(mlp, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO MORE MORE MORE OF ZE CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
