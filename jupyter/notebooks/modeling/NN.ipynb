{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "        https://stackoverflow.com/a/50386871\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import statements\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/processed/full_set.pkl', 'rb') as f:\n",
    "    full_set = pickle.load(f)\n",
    "with open('../../data/processed/train_set.pkl', 'rb') as f:\n",
    "    train_set = pickle.load(f)\n",
    "with open('../../data/processed/test_set.pkl', 'rb') as f:\n",
    "    test_set = pickle.load(f)\n",
    "with open('../../data/processed/train_set_30.pkl', 'rb') as f:\n",
    "    train_set_30 = pickle.load(f)\n",
    "with open('../../data/processed/test_set_30.pkl', 'rb') as f:\n",
    "    test_set_30 = pickle.load(f)\n",
    "with open('../../data/processed/train_set_70.pkl', 'rb') as f:\n",
    "    train_set_70 = pickle.load(f)\n",
    "with open('../../data/processed/test_set_70.pkl', 'rb') as f:\n",
    "    test_set_70 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_70.head()\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X, y format \n",
    "X_train = train_set.T.iloc[:-1].T\n",
    "y_train = train_set.T.iloc[-1].T\n",
    "\n",
    "X_test = test_set.T.iloc[:-1].T\n",
    "y_test = test_set.T.iloc[-1].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with various Neural Network parameters: add or remove nodes, layers and connections, vary the learning rate, epochs and momentum, and validation threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "def get_mlp():\n",
    "    mlp = MLPClassifier(random_state=42\n",
    "                        # more non default parameters? E.g. only stochastic gradient descent has been covered in lectures \n",
    "                        # also doesn't reach convergence before timeout with current settings\n",
    "                       )\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying it using keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "import random\n",
    "import tensorflow as tf\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "from tensorflow import keras\n",
    "# tf.random.set_random_seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "# https://machinelearningmastery.com/build-multi-layer-perceptron-neural-network-models-keras/\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def build_keras_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_dim=2304, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    print(model.summary())\n",
    "\n",
    "    # compile the keras model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fit the keras model on the dataset\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "\n",
    "# evaluate the keras model\n",
    "# loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "kcls = KerasClassifier(build_fn=build_keras_model, epochs=10, batch_size=100)\n",
    "\n",
    "#Note, if you try running this example in an IPython or Jupyter notebook you may get an error.\n",
    "#The reason is the output progress bars during training. \n",
    "#You can easily turn these off by setting verbose=0 in fit() and evaluate() calls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# define 10-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=False, random_state=42)\n",
    "# todo run this\n",
    "def cross_validate(classifier, X, y):\n",
    "    '''    \n",
    "    Given a classifier and training data:\n",
    "        * Do 10fold CV\n",
    "        * average the scores\n",
    "    What this means is for the caller to interpret.\n",
    "    Returns average result over CV runs \n",
    "    '''\n",
    "    standardising_classifier = make_pipeline(preprocessing.StandardScaler(), classifier)\n",
    "    scores = cross_val_score(standardising_classifier, X, y, cv=10)\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running An Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def run_experiment(classifier, X_train_data, y_train_data, X_test_data, y_test_data):\n",
    "    '''\n",
    "    Given a classifier, training data, and test data:\n",
    "    * Train on the training data\n",
    "    * Test on the test data\n",
    "    * display the confusion matrix and other metrics\n",
    "    '''\n",
    "    classifier_pipe = make_pipeline(preprocessing.StandardScaler(), classifier)\n",
    "\n",
    "    classifier_pipe.fit(X_train_data, y_train_data)\n",
    "    y_pred = classifier_pipe.predict(X_test_data)\n",
    "\n",
    "    conf_mat = confusion_matrix(y_test_data, y_pred)\n",
    "    print(classification_report(y_test_data, y_pred))\n",
    "    plot_confusion_matrix(conf_mat, target_names=y_test_data.unique().sort())\n",
    "    \n",
    "    #Conf matrix quantities to give an idea of TP/FP/TN/FN\n",
    "    y_test_data_df = pd.DataFrame(y_test_data)\n",
    "    y_pred_data_df = pd.DataFrame(y_pred)\n",
    "    cnf_matrix = confusion_matrix(y_test_data_df.values, y_pred_data_df.values)\n",
    "    \n",
    "    # https://stackoverflow.com/questions/50666091/true-positive-rate-and-false-positive-rate-tpr-fpr-for-multi-class-data-in-py\n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "    FP = FP.astype(float)\n",
    "    FN = FN.astype(float)\n",
    "    TP = TP.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "    \n",
    "    TPR = TP/(TP+FN)\n",
    "    FPR = FP/(FP+TN)\n",
    "\n",
    "    print(f\"True Positive Rate: {TPR}\\nFalse Positive Rate: {FPR}\\n\")\n",
    "    \n",
    "    #AUC\n",
    "#     print(roc_auc_score(y_test_data, y_pred))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "linclf = SVC(kernel=\"linear\")\n",
    "print(\"Linear Classifier, original split:\")\n",
    "print(f\"{train_set['target'].shape[0]} training examples\")\n",
    "run_experiment(linclf, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "print(\"------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_max = get_mlp()\n",
    "cv = cross_validate(mlp_max, full_set.iloc[:,:-1], full_set['target'])\n",
    "print(cv)\n",
    "# run_experiment(mlp_max, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-7: Using different sized train/test sets instead of cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original split:\")\n",
    "print(f\"{train_set['target'].shape[0]} training examples\")\n",
    "mlp_max = get_mlp()\n",
    "run_experiment(mlp_max, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "print(\"------\\n\")\n",
    "\n",
    "\n",
    "print(\"Reduced training set:\")\n",
    "print(f\"{train_set_30['target'].shape[0]} training examples\")\n",
    "mlp_mid = get_mlp()\n",
    "run_experiment(mlp_mid, train_set_30.iloc[:,:-1], train_set_30['target'], test_set_30.iloc[:,:-1], test_set_30['target'])\n",
    "print(\"------\\n\")\n",
    "\n",
    "\n",
    "print(\"Smallest training set\")\n",
    "print(f\"{train_set_70['target'].shape[0]} training examples\")\n",
    "mlp_small = get_mlp()\n",
    "run_experiment(mlp_small, train_set_70.iloc[:,:-1], train_set_70['target'], test_set_70.iloc[:,:-1], test_set_70['target'])\n",
    "print(\"------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... So the results of the previous cell are a bit too good. Let's keep going with the training set reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# half the remaining training data\n",
    "def half_remaining_training_data(train_X, train_y, test_X, test_y):\n",
    "    X_train_tiny, X_test_tiny_tmp, y_train_tiny, y_test_tiny_tmp = train_test_split(train_X, train_y, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Move the new test split into the existing test data\n",
    "    X_test_tiny = pd.concat([X_test_tiny_tmp, test_X], axis=0)\n",
    "    y_test_tiny = pd.concat([y_test_tiny_tmp, test_y], axis=0)\n",
    "                             \n",
    "    return X_train_tiny, X_test_tiny, y_train_tiny, y_test_tiny\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_90, X_test_90, y_train_90, y_test_90 = half_remaining_training_data(train_set_70.iloc[:,:-1], train_set_70['target'], test_set_70.iloc[:,:-1], test_set_70['target'])\n",
    "X_train_95, X_test_95, y_train_95, y_test_95 = half_remaining_training_data(X_train_90, y_train_90, X_test_90, y_test_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"10% training set\")\n",
    "print(f\"{y_train_90.shape[0]} training examples\")\n",
    "mlp_tiny = get_mlp()\n",
    "run_experiment(mlp_tiny, X_train_90, y_train_90, X_test_90, y_test_90)\n",
    "\n",
    "print(\"5% training set\")\n",
    "print(f\"{y_train_95.shape[0]} training examples\")\n",
    "mlp_tinier = get_mlp()\n",
    "run_experiment(mlp_tinier, X_train_95, y_train_95, X_test_95, y_test_95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Keras model, original data split:\")\n",
    "print(f\"{train_set['target'].shape[0]} training examples\")\n",
    "\n",
    "run_experiment(kcls, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "print(\"------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments in data reshaping to check techniques to be used in the CNN architecture\n",
    "print(train_set.shape)\n",
    "print(X_train_95.shape)\n",
    "X_tmp = train_set.iloc[:,:-1]\n",
    "print(X_tmp.shape)\n",
    "\n",
    "reshaped_train = X_train_95.to_numpy().reshape(-1, 48, 48)\n",
    "print(\"y_train_95.shape: \", y_train_95.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(reshaped_train[0], cmap='Greys')\n",
    "plt.title(y_train_95.iloc[0])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keras_cnn():\n",
    "    '''\n",
    "    A build function for a cnn that can be used by the keras skl classifier wrapper and therefore in our experimental setup.\n",
    "    '''\n",
    "    kernel_size = 3\n",
    "    \n",
    "    inputs = keras.Input(shape=(2304,), name='image')\n",
    "    # reshape to [all examples=> -1, 48*48 image, greyscale=> 1 channel]\n",
    "    reshape = keras.layers.Reshape(target_shape=(48, 48, 1))(inputs)\n",
    "\n",
    "    \n",
    "    layer_1 = keras.layers.Conv2D(filters=32, kernel_size=kernel_size, activation='relu')(reshape)\n",
    "    pool_1 = keras.layers.MaxPooling2D()(layer_1)\n",
    "\n",
    "    dense_block = keras.layers.Flatten()(pool_1)\n",
    "    dense_block = keras.layers.Dense(256, activation='relu')(dense_block)\n",
    "    dense_block = keras.layers.Dense(256, activation='relu')(dense_block)\n",
    "    fin = keras.layers.Dense(64, activation='relu')(dense_block)\n",
    "\n",
    "    # softmax_layer = keras.layers.Dense(10, activation='softmax')(model) # ignore softmax here, use SparseCategoricalCrossentropy below\n",
    "\n",
    "    # compile the keras model\n",
    "    model = keras.Model(inputs=inputs, outputs=fin, name='cnn')\n",
    "\n",
    "    model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tensorboard for visualisations\n",
    "# to view, at the command prompt in this modeling directory with a local tensorflow install:\n",
    "#   tensorboard --logdir tensorboard_logs/fit --host 0.0.0.0\n",
    "# alternatively, see the % tensorboard cell below\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir='tensorboard_logs', histogram_freq=1, write_graph=True, write_images=True\n",
    ")\n",
    "\n",
    "cnn_clf = KerasClassifier(build_fn=build_keras_cnn, epochs=10, batch_size=100, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CNN model, original data split:\")\n",
    "print(f\"{train_set['target'].shape[0]} training examples\")\n",
    "run_experiment(cnn_clf, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "print(\"------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The junk architecture that I threw together as a test is slow but matches performance with the fully connected multi layer perceptron above.\n",
    "Given that reduced training time is supposed to be an advantage of CNNs, this leaves some room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir tensorboard_logs --host 0.0.0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() # View open TensorBoard instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/visualization/\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils import model_to_dot\n",
    "def visualize(mlp2): \n",
    "    # mlp2 = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,\n",
    "    #                     solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "    #                     learning_rate_init=.1)\n",
    "\n",
    "#     mlp2 = get_mlp()\n",
    "#     run_experiment(mlp2, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "\n",
    "    mlp2.fit(train_set.iloc[:,:-1], train_set['target'])\n",
    "    print(\"Training set score: %f\" % mlp2.score(train_set.iloc[:,:-1], train_set['target']))\n",
    "    print(\"Test set score: %f\" % mlp2.score(test_set.iloc[:,:-1], test_set['target']))\n",
    "\n",
    "    fig, axes = plt.subplots(4, 4)\n",
    "    # use global min / max to ensure all weights are shown on the same scale\n",
    "    vmin, vmax = mlp2.coefs_[0].min(), mlp2.coefs_[0].max()\n",
    "    for coef, ax in zip(mlp2.coefs_[0].T, axes.ravel()):\n",
    "        ax.matshow(coef.reshape(48, 48), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
    "                   vmax=.5 * vmax)\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From here on out, will only use full data set since that gave the highest accuracy previously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point of comparison (same code from earlier): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original split:\")\n",
    "print(f\"{train_set['target'].shape[0]} training examples\")\n",
    "mlp_max = get_mlp()\n",
    "run_experiment(mlp_max, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "print(\"------\\n\")\n",
    "# visualize(mlp_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting learning rate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting the type of learning yielded no difference in accuracy value (always 98%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate can be constant, invscaling, adaptive. First and last are straight forward, but:\n",
    "#Invscaling- constant except for when loss has NOT gone down for 2 epochs\n",
    "\n",
    "######### Default init of 0.001 with different methods ########\n",
    "# print(\"Learning Rate Constant: \")\n",
    "# mlp = MLPClassifier(random_state=42, learning_rate='constant')\n",
    "# run_experiment(mlp, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "# print(\"------\\n\")\n",
    "\n",
    "# print(\"Learning Rate Inverse Scaling: \")\n",
    "# mlp = MLPClassifier(random_state=42, learning_rate='invscaling')\n",
    "# run_experiment(mlp, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "# print(\"------\\n\")\n",
    "\n",
    "# print(\"Learning Rate Adaptive: \")\n",
    "# mlp = MLPClassifier(random_state=42, learning_rate='adaptive')\n",
    "# run_experiment(mlp, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "# print(\"------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Different learning rate init values ########\n",
    "\n",
    "#Increase learning rate to 0.5 (learning_rate_init)\n",
    "# print(\"Learning Rate Init = 0.5: \")\n",
    "# mlp = MLPClassifier(random_state=42, learning_rate_init=0.5)\n",
    "# run_experiment(mlp, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "# print(\"------\\n\")\n",
    "\n",
    "#Decrease learning rate to 0.0001 (learning_rate_init)\n",
    "print(\"Learning Rate Init = 0.0001: \")\n",
    "mlp = MLPClassifier(random_state=42, learning_rate_init=0.0001)\n",
    "run_experiment(mlp, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "print(\"------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting Epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs = max_iters in scikit\n",
    "\n",
    "#Increase num epochs\n",
    "print(\"Epochs = 1000\")\n",
    "mlp = MLPClassifier(random_state=42, max_iter=1000)\n",
    "run_experiment(mlp, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "print(\"------\\n\")\n",
    "# visualize(mlp)\n",
    "\n",
    "#Decrease num epochs\n",
    "print(\"Epochs = 5: \")\n",
    "mlp = MLPClassifier(random_state=42, max_iter=5)\n",
    "run_experiment(mlp, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "print(\"------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting Momentum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Between 0 and 1, default = 0.9\n",
    "\n",
    "print(\"Momentum = 1.0: \")\n",
    "mlp = MLPClassifier(random_state=42, momentum=1.0)\n",
    "run_experiment(mlp, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "print(\"------\\n\")\n",
    "\n",
    "print(\"Momentum = 0.5: \")\n",
    "mlp = MLPClassifier(random_state=42, momentum=0.5)\n",
    "run_experiment(mlp, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "print(\"------\\n\")\n",
    "\n",
    "print(\"Momentum = 0.1: \")\n",
    "mlp = MLPClassifier(random_state=42, momentum=0.1)\n",
    "run_experiment(mlp, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "print(\"------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting Validation Threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only used if early_stopping = True (which default is false)\n",
    "#Must be between 0 and 1\n",
    "\n",
    "print(\"Validation Threshold = 0.1: \")\n",
    "mlp = MLPClassifier(random_state=42, early_st./opping=True, validation_fraction=0.1)\n",
    "run_experiment(mlp, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "print(\"------\\n\")\n",
    "\n",
    "print(\"Validation Threshold = 0.5: \")\n",
    "mlp = MLPClassifier(random_state=42, early_stopping=True, validation_fraction=0.5)\n",
    "run_experiment(mlp, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "print(\"------\\n\")\n",
    "\n",
    "print(\"Validation Threshold = 0.9: \")\n",
    "mlp = MLPClassifier(random_state=42, early_stopping=True, validation_fraction=0.9)\n",
    "run_experiment(mlp, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "print(\"------\\n\")\n",
    "\n",
    "\n",
    "# print(\"Validation Threshold = 0.6: \")\n",
    "# mlp = MLPClassifier(random_state=42, early_stopping=True, validation_fraction=0.6)\n",
    "# run_experiment(mlp, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "# print(\"------\\n\")\n",
    "\n",
    "\n",
    "# print(\"Validation Threshold = 0.75: \")\n",
    "# mlp = MLPClassifier(random_state=42, early_stopping=True, validation_fraction=0.75)\n",
    "# run_experiment(mlp, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "# print(\"------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting layers/nodes/connections (via Keras):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "# https://machinelearningmastery.com/build-multi-layer-perceptron-neural-network-models-keras/\n",
    "#96% with orriginal keras model\n",
    "#Baseline was 97% accuracy\n",
    "def two_layer_keras():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_dim=2304, activation='relu'))\n",
    "#     model.add(Dense(256, activation='relu'))\n",
    "#     model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    print(model.summary())\n",
    "\n",
    "    # compile the keras model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fit the keras model on the dataset\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "\n",
    "# evaluate the keras model\n",
    "# loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "kcls_2layers = KerasClassifier(build_fn=two_layer_keras, epochs=10, batch_size=100)\n",
    "run_experiment(kcls_2layers, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "\n",
    "#Note, if you try running this example in an IPython or Jupyter notebook you may get an error.\n",
    "#The reason is the output progress bars during training. \n",
    "#You can easily turn these off by setting verbose=0 in fit() and evaluate() calls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "# https://machinelearningmastery.com/build-multi-layer-perceptron-neural-network-models-keras/\n",
    "#96% with orriginal keras model\n",
    "#Baseline was 97% accuracy\n",
    "def same_layer_keras():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_dim=2304, activation='relu'))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    print(model.summary())\n",
    "\n",
    "    # compile the keras model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fit the keras model on the dataset\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "\n",
    "# evaluate the keras model\n",
    "# loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "kcls_same = KerasClassifier(build_fn=same_layer_keras, epochs=10, batch_size=100)\n",
    "run_experiment(kcls_same, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "\n",
    "#Note, if you try running this example in an IPython or Jupyter notebook you may get an error.\n",
    "#The reason is the output progress bars during training. \n",
    "#You can easily turn these off by setting verbose=0 in fit() and evaluate() calls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "# https://machinelearningmastery.com/build-multi-layer-perceptron-neural-network-models-keras/\n",
    "#96% with orriginal keras model\n",
    "#Baseline was 97% accuracy\n",
    "def six_layer_keras():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_dim=2304, activation='relu'))\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    print(model.summary())\n",
    "\n",
    "    # compile the keras model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fit the keras model on the dataset\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "\n",
    "# evaluate the keras model\n",
    "# loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "kcls_6layers = KerasClassifier(build_fn=six_layer_keras, epochs=10, batch_size=100)\n",
    "run_experiment(kcls_6layers, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "\n",
    "#Note, if you try running this example in an IPython or Jupyter notebook you may get an error.\n",
    "#The reason is the output progress bars during training. \n",
    "#You can easily turn these off by setting verbose=0 in fit() and evaluate() calls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "# https://machinelearningmastery.com/build-multi-layer-perceptron-neural-network-models-keras/\n",
    "#96% with orriginal keras model\n",
    "#Baseline was 97% accuracy\n",
    "def tanh_softmax_keras():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_dim=2304, activation='tanh'))\n",
    "#     model.add(Dense(1024, input_dim=2304, activation='softmax'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    print(model.summary())\n",
    "\n",
    "    # compile the keras model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "kcls_activ = KerasClassifier(build_fn=tanh_softmax_keras, epochs=10, batch_size=100)\n",
    "run_experiment(kcls_activ, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximizing all parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(random_state=42, learning_rate_init=0.0001, max_iter=1000, early_stopping=True, validation_fraction=0.1)\n",
    "run_experiment(mlp, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "print(\"------\\n\")\n",
    "# visualize(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can only work with Keras\n",
    "# history = kcls.fit(train_set.iloc[:,:-1], train_set['target'], validation_split=0.25, epochs=10, batch_size=100)\n",
    "\n",
    "# # Plot training & validation accuracy values\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.title('Model accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# # Plot training & validation loss values\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(historny.history['val_loss'])\n",
    "# plt.title('Model loss')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best performing tests visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback2 = keras.callbacks.TensorBoard(\n",
    "    log_dir='tensorboard_logs_2layers', histogram_freq=1, write_graph=True, write_images=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_clf = KerasClassifier(build_fn=build_keras_cnn, epochs=10, batch_size=100, callbacks=[tensorboard_callback])\n",
    "\n",
    "# mlp_validation = MLPClassifier(random_state=42, early_stopping=True, validation_fraction=0.1, callbacks=[tensorboard_callback])\n",
    "kcls_2layer = KerasClassifier(build_fn=two_layer_keras, epochs=10, batch_size=100, callbacks=[tensorboard_callback2])\n",
    "kcls_1024 = KerasClassifier(build_fn=same_layer_keras, epochs=10, batch_size=100, callbacks=[tensorboard_callback2])\n",
    "kcls_6layer = KerasClassifier(build_fn=six_layer_keras, epochs=10, batch_size=100, callbacks=[tensorboard_callback2])\n",
    "\n",
    "run_experiment(kcls_2layer, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "run_experiment(kcls_1024, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n",
    "run_experiment(kcls_6layer, train_set.iloc[:,:-1], train_set['target'], test_set.iloc[:,:-1], test_set['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir tensorboard_logs_2layers --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
